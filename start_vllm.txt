cat << 'EOF' > /workspace/start_vllm.sh
#!/bin/bash
set -e

echo "Installing dependencies..."
python3 -m pip install --upgrade pip
python3 -m pip install \
  "transformers>=4.57.0" \
  vllm \
  flask==2.2.5 \
  psutil \
  nvidia-ml-py \
  opentelemetry-sdk \
  opentelemetry-exporter-otlp

#######################################
# Start Hardware Metrics Sidecar
#######################################
echo "Starting metrics sidecar (CPU-only)..."
export CUDA_VISIBLE_DEVICES=""
python3 /workspace/my_sidecar.py &
SIDECAR_PID=$!
unset CUDA_VISIBLE_DEVICES

#######################################
# OpenTelemetry (vLLM → Phoenix remoto no Railway)
#######################################
export OTEL_EXPORTER_OTLP_ENDPOINT=https://arize-phoenix-buscafornecedor.up.railway.app/v1/traces
# Auth não é necessário pois PHOENIX_ENABLE_AUTH=False
# export OTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer <PHOENIX_SECRET>"
export OTEL_SERVICE_NAME=vllm-production
export OTEL_METRICS_EXPORTER=none
export OTEL_LOG_LEVEL=debug

#######################################
# Start vLLM (Main Process / PID 1)
#######################################
echo "Starting vLLM..."
exec python3 -m vllm.entrypoints.openai.api_server \
  --host 0.0.0.0 \
  --port 8000 \
  --model mistralai/Ministral-3-3B-Instruct-2512 \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.95 \
  --max-model-len 32000 \
  --kv-cache-dtype fp8 \
  --enforce-eager \
  --trust-remote-code
EOF



chmod +x /workspace/start_vllm.sh

